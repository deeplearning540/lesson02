{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# lesson 02, part 2: Enter  Nearest Neighbors Clustering\n",
    "\n",
    "- unsupervised learning method (no ground truth or labels)\n",
    "\n",
    "## Problem\n",
    "\n",
    "- we already know that our dataset consists of 3 classes: `Adelie, Gentoo, Chinstrap`\n",
    "- let's assume with **DO NOT** have the correct class label for each row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Task \n",
    "\n",
    "- given this dataset \n",
    "$\\mathcal{D} = \\{ \\langle \\vec{x} \\rangle_{i}, i = 1, \\dots, n \\} $\n",
    "- find the $k=3$ clusters to which any of the known points belong to!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Analysis\n",
    "\n",
    "- basis algorithm: finding the nearest point for a query `x_q` given a reference `dataset`\n",
    "  \n",
    "```\n",
    "def bruteforce_nearest_neighbor( x_q, dataset):\n",
    "   closest_point = None\n",
    "   closest_distance = infinity\n",
    "   \n",
    "   for i in range(n):\n",
    "     x_i = dataset[i]\n",
    "     current_distance = distance(x_i, x_q)\n",
    "     if current_distance < closest_distance:\n",
    "       closest_distance = current_distance\n",
    "       closest_point = x_i\n",
    "       \n",
    "   return closest_point\n",
    "```\n",
    "\n",
    "- most common distance metric: Euclidean Distance $d(\\vec{x}_a, \\vec{x}_b) = \\sqrt{ \\sum_{j=1}^{m} (x_{j,a} - x_{j,b})^2 }$\n",
    "- price: \n",
    "  + keep entire \"training set\" in memory\n",
    "  + for each query point, go through entire dataset again (`bruteforce`)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Naive Clustering / Llyod's algorithm\n",
    "\n",
    "goal: create $k$ sets $S$ such as $argmin_{S} \\sum^{k}_{i=1} \\sum_{\\vec{x} \\in S_i} || \\vec{x} - \\vec{\\mu}_i ||^2$\n",
    "\n",
    "\n",
    "algorithm:\n",
    "\n",
    "1. select $k$ points at random and assign them a cluster_id\n",
    "   (consider these points to be the __mean__ of the cluster)\n",
    " \n",
    "![by Weston.pace, from commons.wikimedia.org under CC-BY-SA 3.0](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5e/K_Means_Example_Step_1.svg/249px-K_Means_Example_Step_1.svg.png)\n",
    "   \n",
    "2. assign samples closest to a given cluster mean/centroid so that the variance of the cluster remains minimal\n",
    "\n",
    "![by Weston.pace, from commons.wikimedia.org under CC-BY-SA 3.0](https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/K_Means_Example_Step_2.svg/278px-K_Means_Example_Step_2.svg.png)\n",
    "\n",
    "3. calculate the distance of all points to those cluster means (also called centroids) and update the mean cluster centroid for each cluster\n",
    "\n",
    "![by Weston.pace, from commons.wikimedia.org under CC-BY-SA 3.0](https://upload.wikimedia.org/wikipedia/commons/thumb/3/3e/K_Means_Example_Step_3.svg/278px-K_Means_Example_Step_3.svg.png)\n",
    "\n",
    "4. Steps 2 and 3 are repeated until convergence has been reached, i.e. the cluster association does not change anymore.\n",
    "\n",
    "![by Weston.pace, from commons.wikimedia.org under CC-BY-SA 3.0](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d2/K_Means_Example_Step_4.svg/278px-K_Means_Example_Step_4.svg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#using kMeans cluster from scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Further Reading\n",
    "\n",
    "- some parts of this material were inspired by the ever awesome [Sebastian Raschka](https://sebastianraschka.com)\n",
    "  + general overview of machine learning [lesson 01](https://sebastianraschka.com/resources/ml-lectures-1.html#l01-what-is-machine-learning)\n",
    "  + nearest neighbor methods [lesson 02](https://sebastianraschka.com/resources/ml-lectures-1.html/#l02-nearest-neighbor-methods)\n",
    "  \n",
    "- the [wikipedia page on kmeans clustering](https://en.wikipedia.org/wiki/K-means_clustering) is well written too- plot the [decision boundary of the clustering](https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_decision_regions.html#sphx-glr-auto-examples-ensemble-plot-voting-decision-regions-py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": null,
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": null,
   "name": "python3"
  },
  "name": "part2.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
